{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b301f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h1(x):\n",
    "    if(x[0] > -0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1 \n",
    "    \n",
    "def h2(x):\n",
    "    if(x[0] > -0.5):\n",
    "        return -1 \n",
    "    else:\n",
    "        return 1 \n",
    "\n",
    "def h3(x):\n",
    "    if(x[0] > 0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1 \n",
    "    \n",
    "def h4(x):\n",
    "    if(x[0] > 0.5):\n",
    "        return -1 \n",
    "    else:\n",
    "        return 1 \n",
    "    \n",
    "def h5(x):\n",
    "    if(x[1] > -0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1 \n",
    "    \n",
    "def h6(x):\n",
    "    if(x[1] > -0.5):\n",
    "        return -1 \n",
    "    else:\n",
    "        return 1 \n",
    "    \n",
    "def h7(x):\n",
    "    if(x[1] > 0.5):\n",
    "        return 1 \n",
    "    else:\n",
    "        return -1 \n",
    "    \n",
    "def h8(x):\n",
    "    if(x[1] > 0.5):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7c9fbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def training_error(samples, y, W):\n",
    "    h = ['h1','h2','h3','h4','h5','h6','h7','h8']\n",
    "    error = []\n",
    "    for h_i in h:\n",
    "        count = 0\n",
    "        for i,sample in enumerate(samples):\n",
    "            #print(sample)\n",
    "            if(y[i] != globals()[h_i](sample)):\n",
    "                # misclassified\n",
    "                count = count + W[i]\n",
    "        error.append(round(count,4))\n",
    "    #return [error_i / len(samples) for error_i in error]\n",
    "    return error\n",
    "\n",
    "\n",
    "\n",
    "def adaboost(x,y, k_max = 1):\n",
    "    chosen_h = []\n",
    "    alphas = []\n",
    "    n = len(x)\n",
    "    W = [1/n for i in range(0, len(x))]\n",
    "    h = ['h1','h2','h3','h4','h5','h6','h7','h8']\n",
    "    for k in range(0, k_max):\n",
    "        print(\"> iteration : \", k+1)\n",
    "        E = training_error(x, y, W)\n",
    "        print(\"> training errors for all classifiers: \", E)\n",
    "        h_i = E.index(min(E))\n",
    "        print(\"> chosen min classifier h^k = \", h_i+1)\n",
    "        chosen_h.append(h_i)\n",
    "        epi = min(E)\n",
    "        print(\"> minimum training error e = \", epi)\n",
    "        alpha = round(1/2 * math.log((1-epi)/epi),4)\n",
    "        print(\"> alpha = \", alpha)\n",
    "        alphas.append(alpha)\n",
    "        for i in range(0,len(W)):\n",
    "            W[i] = W[i] * round(math.exp(-alpha*y[i]*globals()[h[h_i]](x[i])),4)\n",
    "        z = round(sum(W),4)\n",
    "        \n",
    "        W = [round(w_i/z,4) for w_i in W]\n",
    "        print(\"updated weights for all samples: \", W)\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "761a86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> iteration :  1\n",
      "> training errors for all classifiers:  [0.75, 0.25, 0.25, 0.75, 0.25, 0.75, 0.75, 0.25]\n",
      "> chosen min classifier h^k =  2\n",
      "> minimum training error e =  0.25\n",
      "> alpha =  0.5493\n",
      "updated weights for all samples:  [0.5, 0.1667, 0.1667, 0.1667]\n",
      "\n",
      "\n",
      "> iteration :  2\n",
      "> training errors for all classifiers:  [0.5001, 0.5, 0.1667, 0.8334, 0.1667, 0.8334, 0.8334, 0.1667]\n",
      "> chosen min classifier h^k =  3\n",
      "> minimum training error e =  0.1667\n",
      "> alpha =  0.8046\n",
      "updated weights for all samples:  [0.3, 0.4999, 0.1, 0.1]\n",
      "\n",
      "\n",
      "> iteration :  3\n",
      "> training errors for all classifiers:  [0.6999, 0.3, 0.4999, 0.5, 0.1, 0.8999, 0.8999, 0.1]\n",
      "> chosen min classifier h^k =  5\n",
      "> minimum training error e =  0.1\n",
      "> alpha =  1.0986\n",
      "updated weights for all samples:  [0.1667, 0.2777, 0.5001, 0.0556]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,0],\n",
    "              [-1,0],\n",
    "              [0,1],\n",
    "              [0,-1]])\n",
    "\n",
    "y = np.array([1,1,-1,-1])\n",
    "k_max = 3\n",
    "adaboost(x, y, k_max = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46825877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_error(y, W):\n",
    "    h = np.array([[1,-1,1,1,1],\n",
    "         [-1,1,-1,1,1],\n",
    "         [-1,-1,1,-1,-1]])\n",
    "    error = []\n",
    "    for i in h:\n",
    "        count = 0\n",
    "        for index, hi in enumerate(i):\n",
    "            if(y[index] != hi):\n",
    "                # misclassified\n",
    "                count = count + W[index]\n",
    "        error.append(round(count,4))\n",
    "    #return [error_i / len(samples) for error_i in error]\n",
    "    \n",
    "    return error\n",
    "\n",
    "def adaboost(y, k_max = 1):\n",
    "    chosen_h = []\n",
    "    alphas = []\n",
    "    n = len(y)\n",
    "    W = [1/n for i in range(0, len(y))]\n",
    "    h = np.array([[1,-1,1,1,1],\n",
    "         [-1,1,-1,1,1],\n",
    "         [-1,-1,1,-1,-1]])\n",
    "    for k in range(0, k_max):\n",
    "        print(\"> iteration : \", k+1)\n",
    "        E = training_error(y, W)\n",
    "        print(\"> training errors for all classifiers: \", E)\n",
    "        h_i = E.index(min(E))\n",
    "        print(\"> chosen min classifier h^k = \", h_i+1)\n",
    "        chosen_h.append(h_i)\n",
    "        epi = min(E)\n",
    "        print(\"> minimum training error e = \", epi)\n",
    "        alpha = round(1/2 * math.log((1-epi)/epi),4)\n",
    "        print(\"> alpha = \", alpha)\n",
    "        alphas.append(alpha)\n",
    "        #for i in range(0,len(W)):\n",
    "        #    W[i] = W[i] * round(math.exp(-alpha*y[i]*globals()[h[h_i]](x[i])),4)\n",
    "        #z = round(sum(W),4)\n",
    "        \n",
    "        #W = [round(w_i/z,4) for w_i in W]\n",
    "        #print(\"updated weights for all samples: \", W)\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "15358d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> iteration :  1\n",
      "> training errors for all classifiers:  [0.2, 0.4, 0.4]\n",
      "> chosen min classifier h^k =  1\n",
      "> minimum training error e =  0.2\n",
      "> alpha =  0.6931\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "k_max = 1\n",
    "adaboost(y, k_max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c33995be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> iteration :  1\n",
      "> training errors for all classifiers:  [0.75, 0.25, 0.25, 0.75, 0.25, 0.75, 0.75, 0.25]\n",
      "> chosen min classifier h^k =  2\n",
      "> minimum training error e =  0.25\n",
      "> alpha =  0.5493\n",
      "updated weights for all samples:  [0.5, 0.1667, 0.1667, 0.1667]\n",
      "\n",
      "\n",
      "> iteration :  2\n",
      "> training errors for all classifiers:  [0.5001, 0.5, 0.1667, 0.8334, 0.1667, 0.8334, 0.8334, 0.1667]\n",
      "> chosen min classifier h^k =  3\n",
      "> minimum training error e =  0.1667\n",
      "> alpha =  0.8046\n",
      "updated weights for all samples:  [0.3, 0.4999, 0.1, 0.1]\n",
      "\n",
      "\n",
      "> iteration :  3\n",
      "> training errors for all classifiers:  [0.6999, 0.3, 0.4999, 0.5, 0.1, 0.8999, 0.8999, 0.1]\n",
      "> chosen min classifier h^k =  5\n",
      "> minimum training error e =  0.1\n",
      "> alpha =  1.0986\n",
      "updated weights for all samples:  [0.1667, 0.2777, 0.5001, 0.0556]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Â MAKE SURE TO ALTER THE NUMBER OF CLASSIFIERS\n",
    "adaboost(x,y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a559f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
